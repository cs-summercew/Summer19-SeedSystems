TODO:
dummy variables
when to scale
overview of types of Supervised ML: 
    output: Regression vs Classification
    input: Dummy(Male/Female) <=> Categorical(number of children, number of cylinders), Continuous(weight), mixed

OneHot Encoding/Dummy Variables (Dealing with Categorical Data):
https://blog.myyellowroad.com/using-categorical-data-in-machine-learning-with-python-from-dummy-variables-to-deep-category-66041f734512
LabelEncoder or pd.get_dummies

Model Evaluation:
https://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-log-error

Preprocessing Data:
https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features

Temporary file for README information
So you want to learn Machine Learning ehh? Well I've collected here all the knowledge I've gathered across many moons and countless hours reading tutorials. Use wisely

TLDR for multicollinearity:
Having multicollinear variables isn't too big a worry for your ML models unless you care about having meaningful regression coefficients. However, it does seem that if your variables are REALLY multicollinear, then there may be some problems. Regardless, its probably still a good practice to at least check.

Useful Websites for ML topics:

General:
https://blog.minitab.com/blog/adventures-in-statistics-2/regression-analysis-tutorial-and-examples
https://machinelearningmastery.com
http://blog.datadive.net


Feature Selection:
http://blog.datadive.net/selecting-good-features-part-i-univariate-selection/
https://machinelearningmastery.com/feature-selection-machine-learning-python/

Mixed Categorical and numeric features:
https://stackoverflow.com/questions/40434485/machine-learning-algorithm-for-mixed-categorical-and-numeric-features


What if you have only categorical features?
Refer to Classification-Pipeline.py

What if you have only numeric(continuous) features?
Refer to Regression-Pipeline.py

What if you have a mix of the two kinds of features?
Refer to Regression-Pipeline.py
Options:
1- Don't use one type of feature (my approach in Regression-Pipeline.py)
2- Put ranges of your numeric(continuous) into bins
3- Perform Normalization: for each feature, subtract the mean of the feature and then divide by the standard deviation of the feature. Use StandardScaler in Scikit-learn.